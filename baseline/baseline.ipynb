{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks you through the implementation of the tagger–parser pipeline that serves as the baseline for the [standard project](https://www.ida.liu.se/~TDDE09/project/standard-project.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline system can work with any treebank released by the [Universal Dependencies Project](http://universaldependencies.org). To read a treebank, we use the [CoNLL-U Parser](https://pypi.org/project/conllu/) library. The code in the next cell defines a PyTorch [Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) wrapper for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Treebank(Dataset):\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        super().__init__()\n",
    "        self.items = []\n",
    "        with open(filename, 'rt', encoding='utf-8') as fp:\n",
    "            for tokens in conllu.parse_incr(fp):\n",
    "                sentence = [('[ROOT]', '[ROOT]', 0)]\n",
    "                for token in tokens.filter(id=lambda x: type(x) is int):\n",
    "                    sentence.append((token['form'], token['upos'], token['head']))\n",
    "                self.items.append(sentence)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.items[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the training data and the development data from the English Web Treebank. Because the arc-standard algorithm is restricted to projective dependency trees, we need a projectivised version of the training data; this version can be produced using the script `projectivize.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = Treebank('en_ewt-ud-train-projectivized.conllu')\n",
    "\n",
    "DEV_DATA = Treebank('en_ewt-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data consists of **parsed sentences**. A parsed sentence is represented as a list of triples. The first component of each triple (a string) represents a word. The second component (a string) specifies its part-of-speech tag; the possible tags are listed in the [Annotation Guidelines](http://universaldependencies.org/u/pos/all.html) of the Universal Dependencies Project. The third component of each triple (an integer) specifies the position of the word’s head, i.e., its parent in the dependency tree.\n",
    "\n",
    "Run the next cell to see an example sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA[531]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we prefix each sentence with a special `[ROOT]` token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Vocabularies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline uses two vocabularies: one for the words and one for the tags. Both are represented as dictionaries that map words/tags to a contiguous range of integers, starting at zero.\n",
    "\n",
    "The next cell contains code for a function `make_vocabs` that constructs the two vocabularies from gold-standard data. The code cell also defines a name for the “unknown word” (`[UNK]`) and for an additional pseudoword that serves as a placeholder for undefined values (`[PAD]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '[PAD]'\n",
    "UNK = '[UNK]'\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def make_vocabs(gold_data):\n",
    "    vocab_words = {PAD: PAD_IDX, UNK: UNK_IDX}\n",
    "    vocab_tags = {PAD: PAD_IDX}\n",
    "    for sentence in gold_data:\n",
    "        for word, tag, _ in sentence:\n",
    "            if word not in vocab_words:\n",
    "                vocab_words[word] = len(vocab_words)\n",
    "            if tag not in vocab_tags:\n",
    "                vocab_tags[tag] = len(vocab_tags)\n",
    "    return vocab_words, vocab_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Fixed-window model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the tagger and the parser of the baseline system use a fixed-window model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic structure\n",
    "\n",
    "An input to the fixed-window model takes the form of a $k$-dimensional vector of word ids and/or tag ids. Each integer $i$ is mapped to an $e_i$-dimensional embedding vector. These vectors are concatenated to form a vector of length $e_1 + \\cdots + e_k$, and sent through a feed-forward network with a single hidden layer followed by a rectified linear unit (ReLU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding specifications\n",
    "\n",
    "To make our implementation of the fixed-window model useful for both the tagger and the parser, it can be configured with *embedding specifications*. An embedding specification is a triple $(m, n, e)$ consisting of three integers. Such a triple specifies that the model should set up $m$ instances of an embedding from $n$ items to vectors of size $e$. All of the $m$ instances share their weights. For example, to instantiate the default feature model of the tagger (see below), we initialise the model with the following specifications:\n",
    "\n",
    "``\n",
    "[(3, num_words, word_dim), (1, num_tags, tag_dim)]\n",
    "``\n",
    "\n",
    "This specifies that the model should use 3 instances of an embedding from *num_words* words to vectors of length *word_dim*, and 1 instance of an embedding from *num_tags* tags to vectors of length *tag_dim*. All 3 instances of the word embedding should share their weights.\n",
    "\n",
    "We initialise the weights of each embedding with values drawn from $\\mathcal{N}(0, 10^{-2})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specification of the fixed-window model\n",
    "\n",
    "Here is the specification of the fixed-window model interface:\n",
    "\n",
    "**__init__** (*self*, *embedding_specs*, *hidden_dim*, *output_dim*)\n",
    "\n",
    "> A fixed-window model is initialised with a list of specifications for the embeddings the network should use (*embedding_specs*), the size of the hidden layer (*hidden_dim*), and the size of the output layer (*output_dim*).\n",
    "\n",
    "**forward** (*self*, *features*)\n",
    "\n",
    "> Computes the network output for a given feature representation *features*. This is a tensor of shape $B \\times k$ where $B$ is the batch size (number of samples in the batch) and $k$ is the total number of embeddings specified upon initialisation. For example, for the default feature model, $k=4$, as this model includes 3 (weight-sharing) word embeddings and 1 tag embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FixedWindowModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_specs, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create the embeddings based on the given specifications\n",
    "        self.embeddings = nn.ModuleList()\n",
    "        for n, num_embeddings, embedding_dim in embedding_specs:\n",
    "            embedding = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
    "            nn.init.normal_(embedding.weight, std=1e-2)\n",
    "            for i in range(n):\n",
    "                self.embeddings.append(embedding)\n",
    "\n",
    "        # Set up the FFN\n",
    "        input_dim = sum(e.embedding_dim for e in self.embeddings)\n",
    "        self.pipe = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = [e(x[..., i]) for i, e in enumerate(self.embeddings)]\n",
    "        return self.pipe(torch.cat(embedded, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Part-of-speech tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tagger is a simple auto-regressive tagger that processes an input sentence from left to right, and at each position, predicts the tag for the current word based on the features extracted from the current feature window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagger interface\n",
    "\n",
    "The tagger implements a very simple interface:\n",
    "\n",
    "**predict** (*self*, *sentence*)\n",
    "\n",
    "> Returns the list of predicted tags (a list of strings) for a single *sentence* (a list of string tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tagger(object):\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default feature model\n",
    "\n",
    "The default feature model of the tagger has the following features ($k=4$):\n",
    "\n",
    "0. current word\n",
    "1. previous word\n",
    "2. next word\n",
    "3. tag predicted for the previous word\n",
    "\n",
    "Whenever the value of a feature is undefined, we use the special value `[PAD]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specification of the implementation\n",
    "\n",
    "Here is the specification of the tagger implementation:\n",
    "\n",
    "**__init__** (*self*, *vocab_words*, *vocab_tags*, *word_dim* = 50, *tag_dim* = 10, *hidden_dim* = 100)\n",
    "\n",
    "> Creates a new fixed-window model of appropriate dimensions and registers the vocabularies. The parameters *vocab_words* and *vocab_tags* are the word vocabulary and tag vocabulary. The parameters *word_dim* and *tag_dim* specify the embedding width for the word embeddings and tag embeddings.\n",
    "\n",
    "**featurize** (*self*, *words*, *i*, *pred_tags*)\n",
    "\n",
    "> Extracts features from the specified tagger configuration according to the default feature model. The configuration is specified in terms of the words in the input sentence (*words*, a list of word ids), the position of the current word (*i*), and the list of already predicted tags (*pred_tags*, a list of tag ids). Returns a tensor that can be fed to the fixed-window model.\n",
    "\n",
    "**predict** (*self*, *words*)\n",
    "\n",
    "> Processes the input sentence *words* (a list of string tokens) and makes calls to the fixed-window model to predict the tag of each word. Returns the list of the predicted tags (strings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowTagger(Tagger):\n",
    "\n",
    "    def __init__(self, vocab_words, vocab_tags, word_dim=50, tag_dim=10, hidden_dim=100):\n",
    "        embedding_specs = [(3, len(vocab_words), word_dim), (1, len(vocab_tags), tag_dim)]\n",
    "        self.model = FixedWindowModel(embedding_specs, hidden_dim, len(vocab_tags))\n",
    "        self.w2i = vocab_words\n",
    "        self.i2t = {i: t for t, i in vocab_tags.items()}\n",
    "\n",
    "    def featurize(self, words, i, pred_tags):\n",
    "        x = torch.zeros(4, dtype=torch.long)\n",
    "        x[0] = words[i]\n",
    "        x[1] = words[i - 1] if i > 0 else PAD_IDX\n",
    "        x[2] = words[i + 1] if i + 1 < len(words) else PAD_IDX\n",
    "        x[3] = pred_tags[i - 1] if i > 0 else PAD_IDX\n",
    "        return x\n",
    "\n",
    "    def predict(self, words):\n",
    "        words = [self.w2i.get(w, UNK_IDX) for w in words]\n",
    "        pred_tags = []\n",
    "        for i in range(len(words)):\n",
    "            features = self.featurize(words, i, pred_tags)\n",
    "            with torch.no_grad():\n",
    "                scores = self.model.forward(features)\n",
    "            pred_tag = scores.argmax().item()\n",
    "            pred_tags.append(pred_tag)\n",
    "        return [self.i2t[i] for i in pred_tags]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the training examples\n",
    "\n",
    "To generate the training examples for the tagger, we use the following generator function:\n",
    "\n",
    "**training_examples** (*vocab_words*, *vocab_tags*, *gold_data*, *tagger*, *batch_size* = 100)\n",
    "\n",
    "> Iterates through the given *gold_data* (an iterable of parsed sentences), encodes it into word ids and tag ids using the specified vocabularies *vocab_words* and *vocab_tags*, and then yields batches of training examples for gradient-based training. Each batch contains *batch_size* examples, except for the last batch, which may contain fewer examples. Each example in the batch is created by a call to the `featurize` function of the *tagger*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def training_examples(vocab_words, vocab_tags, gold_data, tagger, batch_size=100, shuffle=False):\n",
    "    bx = []\n",
    "    by = []\n",
    "    for sentence in gold_data:\n",
    "        # Separate the words and the gold-standard tags\n",
    "        words, gold_tags, _ = zip(*sentence)\n",
    "\n",
    "        # Encode words and tags using the vocabularies\n",
    "        words = [vocab_words.get(w, UNK_IDX) for w in words]\n",
    "        gold_tags = [vocab_tags[t] for t in gold_tags]\n",
    "\n",
    "        # Simulate a run of the tagger over the sentence, collecting training examples\n",
    "        pred_tags = []\n",
    "        for i, gold_tag in enumerate(gold_tags):\n",
    "            bx.append(tagger.featurize(words, i, pred_tags))\n",
    "            by.append(gold_tag)\n",
    "            if len(bx) >= batch_size:\n",
    "                bx = torch.stack(bx)\n",
    "                by = torch.LongTensor(by)\n",
    "                if shuffle:\n",
    "                    random_indices = torch.randperm(len(bx))\n",
    "                    yield bx[random_indices], by[random_indices]\n",
    "                else:\n",
    "                    yield bx, by\n",
    "                bx = []\n",
    "                by = []\n",
    "            pred_tags.append(gold_tag)    # teacher forcing!\n",
    "\n",
    "    # Check whether there is an incomplete batch\n",
    "    if bx:\n",
    "        bx = torch.stack(bx)\n",
    "        by = torch.LongTensor(by)\n",
    "        if shuffle:\n",
    "            random_indices = torch.randperm(len(bx))\n",
    "            yield bx[random_indices], by[random_indices]\n",
    "        else:\n",
    "            yield bx, by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "Training the tagger uses a straightforward training loop.\n",
    "\n",
    "**train_tagger** (*train_data*, *n_epochs* = 1, *batch_size* = 100, *lr* = 1e-2)\n",
    "\n",
    "> Trains a fixed-window tagger from a set of training data *train_data* (an iterable over parsed sentences) using minibatch gradient descent and returns it. The parameters *n_epochs* and *batch_size* specify the number of training epochs and the minibatch size, respectively. Training uses the cross-entropy loss function and the [Adam optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) with learning rate *lr*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_tagger(train_data, n_epochs=1, batch_size=100, lr=1e-2):\n",
    "    # Create the vocabularies\n",
    "    vocab_words, vocab_tags = make_vocabs(train_data)\n",
    "\n",
    "    # Instantiate the tagger\n",
    "    tagger = FixedWindowTagger(vocab_words, vocab_tags)\n",
    "\n",
    "    # Instantiate the optimizer\n",
    "    optimizer = optim.Adam(tagger.model.parameters(), lr=lr)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0\n",
    "        n_examples = 0\n",
    "        with tqdm(total=sum(len(s) for s in train_data)) as pbar:\n",
    "            for bx, by in training_examples(vocab_words, vocab_tags, train_data, tagger):\n",
    "                optimizer.zero_grad()\n",
    "                output = tagger.model.forward(bx)\n",
    "                loss = F.cross_entropy(output, by)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                n_examples += 1\n",
    "                pbar.set_postfix(loss=running_loss/n_examples)\n",
    "                pbar.update(len(bx))\n",
    "\n",
    "    return tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation function\n",
    "\n",
    "To evaluate a tagger, we compute its per-token accuracy.\n",
    "\n",
    "**accuracy** (*tagger*, *gold_data*)\n",
    "\n",
    "> Computes the accuracy of the *tagger* on the gold-standard data *gold_data* (an iterable of parsed sentences) and returns it as a float. Recall that the accuracy is defined as the percentage of tokens to which the tagger assigns the correct tag (as per the gold standard). The calculation ignores the pseudo-root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(tagger, gold_data):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for sentence in gold_data:\n",
    "        words, gold_tags, _ = zip(*sentence)\n",
    "        pred_tags = tagger.predict(words)\n",
    "        for gold_tag, pred_tag in zip(gold_tags[1:], pred_tags[1:]):  # ignore the pseudo-root\n",
    "            correct += int(gold_tag == pred_tag)\n",
    "            total += 1\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "The next code cell trains a tagger and evaluates it on the development data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAGGER = train_tagger(TRAIN_DATA)\n",
    "print('{:.4f}'.format(accuracy(TAGGER, DEV_DATA)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tagging accuracy on the development data should be around 88%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parser part of the baseline system is a dependency parser based on the arc-standard algorithm. It consists of two parts: one that implements the algorithm logic and one that encapsulates the learning component – the fixed-window model. The parser uses the fixed-window model to predict the next move for a given configuration in the arc-standard algorithm, based on the features extracted from the current feature window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser interface\n",
    "\n",
    "Like the tagger, the parser has a very simple interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(object):\n",
    "\n",
    "    def predict(self, words, tags):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single method of this interface has the following specification:\n",
    "\n",
    "**predict** (*self*, *words*, *tags*)\n",
    "\n",
    "> Returns the list of predicted heads (a list of integers) for a single sentence, specified in terms of its *words* (a list of strings) and their corresponding *tags* (also a list of strings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default feature model\n",
    "\n",
    "For the parser, we will use the following features ($k=6$):\n",
    "\n",
    "0. word form of the next word in the buffer\n",
    "1. word form of the topmost word on the stack\n",
    "2. word form of the second-topmost word on the stack\n",
    "3. part-of-speech tag of the next word in the buffer\n",
    "4. part-of-speech tag of the topmost word on the stack\n",
    "5. part-of-speech tag of the second-topmost word on the stack\n",
    "\n",
    "Whenever the value of a feature is undefined, you should use the special value `PAD`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arc-standard algorithm\n",
    "\n",
    "Recall that, in the arc-standard algorithm, the next move (also called “transition”) of the parser is predicted based on features extracted from the current parser configuration, with references to the words and part-of-speech tags of the input sentence. On the Python side of things, the words and part-of-speech tags are represented as lists of strings, and a configuration is represented as a triple\n",
    "\n",
    "$$\n",
    "(i, \\mathit{stack}, \\mathit{heads})\n",
    "$$\n",
    "\n",
    "where $i$ is an integer specifying the position of the next word in the buffer, $\\mathit{stack}$ is a list of integers specifying the positions of the words currently on the stack (with the topmost element last in the list), and $\\mathit{heads}$ is a list of integers specifying the positions of the head words. If a word has not yet been assigned a head, its head value is&nbsp;0. To illustrate this representation, the initial configuration for the example sentence is\n",
    "\n",
    "(0, [], [0, 0, 0, 0, 0, 0])\n",
    "\n",
    "and a possible final configuration is\n",
    "\n",
    "(6, [0], [0, 2, 0, 4, 2, 2])\n",
    "\n",
    "In the lecture, both the buffer and the stack were presented as list of words. Here we only represent the *stack* as a list of words. To represent the *buffer*, we simply record the position of the next word that has not been processed yet (the integer $i$). This acknowledges the fact that the buffer (in contrast to the stack) can never grow, but will be processed from left to right.\n",
    "\n",
    "Here is the specification of the implementation of the algorithmic part of the parser:\n",
    "\n",
    "**initial_config** (*num_words*)\n",
    "\n",
    "> Returns the initial configuration for a sentence with the specified number of words (*num_words*).\n",
    "\n",
    "**valid_moves** (*config*)\n",
    "\n",
    "> Returns the list of valid moves for the specified configuration (*config*).\n",
    "\n",
    "**next_config** (*config*, *move*)\n",
    "\n",
    "> Applies the *move* in the specified configuration *config* and returns the new configuration. This must not modify the input configuration.\n",
    "\n",
    "**is_final_config** (*config*)\n",
    "\n",
    "> Tests whether *config* is a final configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcStandardParser(Parser):\n",
    "\n",
    "    MOVES = tuple(range(3))\n",
    "\n",
    "    SH, LA, RA = MOVES\n",
    "\n",
    "    @staticmethod\n",
    "    def initial_config(num_words):\n",
    "        return 0, [], [0] * num_words\n",
    "\n",
    "    @staticmethod\n",
    "    def valid_moves(config):\n",
    "        pos, stack, heads = config\n",
    "        moves = []\n",
    "        if pos < len(heads):\n",
    "            moves.append(ArcStandardParser.SH)\n",
    "        if len(stack) >= 3:  # disallow LA with root as dependent\n",
    "            moves.append(ArcStandardParser.LA)\n",
    "        if len(stack) >= 2:\n",
    "            moves.append(ArcStandardParser.RA)\n",
    "        return moves\n",
    "\n",
    "    @staticmethod\n",
    "    def next_config(config, move):\n",
    "        pos, stack, heads = config\n",
    "        stack = list(stack)  # copy because we will modify it\n",
    "        if move == ArcStandardParser.SH:\n",
    "            stack.append(pos)\n",
    "            pos += 1\n",
    "        else:\n",
    "            heads = list(heads)  # copy because we will modify it\n",
    "            s1 = stack.pop()\n",
    "            s2 = stack.pop()\n",
    "            if move == ArcStandardParser.LA:\n",
    "                heads[s2] = s1\n",
    "                stack.append(s1)\n",
    "            if move == ArcStandardParser.RA:\n",
    "                heads[s1] = s2\n",
    "                stack.append(s2)\n",
    "        return pos, stack, heads\n",
    "\n",
    "    @staticmethod\n",
    "    def is_final_config(config):\n",
    "        pos, stack, heads = config\n",
    "        return pos == len(heads) and len(stack) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specification of the implementation\n",
    "\n",
    "Here is the specification of the parser implementation:\n",
    "\n",
    "**__init__** (*self*, *vocab_words*, *vocab_tags*, *word_dim* = 50, *tag_dim* = 10, *hidden_dim* = 100)\n",
    "\n",
    "> Creates a new fixed-window model of appropriate dimensions and sets up any other data structures that you consider relevant. The parameters *vocab_words* and *vocab_tags* are the word vocabulary and tag vocabulary. The parameters *word_dim* and *tag_dim* specify the embedding width for the word embeddings and tag embeddings.\n",
    "\n",
    "**featurize** (*self*, *words*, *tags*, *config*)\n",
    "\n",
    "> Extracts features from the specified parser state according to the feature model given above. The state is specified in terms of the words in the input sentence (*words*, a list of word ids), their part-of-speech tags (*tags*, a list of tag ids), and the parser configuration proper (*config*, as specified in Problem&nbsp;3).\n",
    "\n",
    "**predict** (*self*, *words*, *tags*)\n",
    "\n",
    "> Predicts the list of all heads for the input sentence. This simulates the arc-standard algorithm, calling the move classifier whenever it needs to take a decision. The input sentence is specified in terms of the list of its words (strings) and the list of its tags (strings). Both of these should include the pseudoroot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowParser(ArcStandardParser):\n",
    "\n",
    "    def __init__(self, vocab_words, vocab_tags, word_dim=50, tag_dim=10, hidden_dim=180):\n",
    "        embedding_specs = [(3, len(vocab_words), word_dim), (3, len(vocab_tags), tag_dim)]\n",
    "        self.model = FixedWindowModel(embedding_specs, hidden_dim, len(ArcStandardParser.MOVES))\n",
    "        self.w2i = vocab_words\n",
    "        self.t2i = vocab_tags\n",
    "\n",
    "    def featurize(self, words, tags, config):\n",
    "        i, stack, heads = config\n",
    "        x = torch.zeros(6, dtype=torch.long)\n",
    "        x[0] = words[i] if i < len(words) else PAD_IDX\n",
    "        x[1] = words[stack[-1]] if len(stack) >= 1 else PAD_IDX\n",
    "        x[2] = words[stack[-2]] if len(stack) >= 2 else PAD_IDX\n",
    "        x[3] = tags[i] if i < len(tags) else PAD_IDX\n",
    "        x[4] = tags[stack[-1]] if len(stack) >= 1 else PAD_IDX\n",
    "        x[5] = tags[stack[-2]] if len(stack) >= 2 else PAD_IDX\n",
    "        return x\n",
    "\n",
    "    def predict(self, words, tags):\n",
    "        words = [self.w2i.get(w, UNK_IDX) for w in words]\n",
    "        tags = [self.t2i.get(t, UNK_IDX) for t in tags]\n",
    "        config = self.initial_config(len(words))\n",
    "        valid_moves = self.valid_moves(config)\n",
    "        while valid_moves:\n",
    "            features = self.featurize(words, tags, config)\n",
    "            with torch.no_grad():\n",
    "                scores = self.model.forward(features)\n",
    "\n",
    "            # We may only predict valid transitions\n",
    "            best_score, pred_move = float('-inf'), None\n",
    "            for move in valid_moves:\n",
    "                if scores[move] > best_score:\n",
    "                    best_score, pred_move = scores[move], move\n",
    "\n",
    "            config = self.next_config(config, pred_move)\n",
    "            valid_moves = self.valid_moves(config)\n",
    "        i, stack, pred_heads = config\n",
    "        return pred_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oracle\n",
    "\n",
    "The learning component of the parser is the next move classifier. To train this classifier, we need training examples of the form $(\\mathbf{x}, m)$, where $\\mathbf{x}$ is a feature vector extracted from a given parser configuration $c$, and $m$ is the corresponding gold-standard move. To obtain $m$, we need an **oracle**.\n",
    "\n",
    "Recall that, in the context of transition-based dependency parsing, an oracle is a function that translates a gold-standard dependency tree (here represented as a list of head ids) into a sequence of moves such that, when the parser takes the moves starting from the initial configuration, then it recreates the original dependency tree. \n",
    "\n",
    "Here is the formal specification of the oracle:\n",
    "\n",
    "**oracle_moves** (*gold_heads*)\n",
    "\n",
    "> Translates a gold-standard head assignment for a single sentence (*gold_heads*) into the corresponding stream of oracle moves. More specifically, this yields pairs $(c, m)$ where $m$ is a move (an integer, as specified in the `ArcStandardParser` interface) and $c$ is the parser configuration in which $m$ was taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oracle_moves(gold_heads):\n",
    "    # Keep track of how many dependents each head still needs to find\n",
    "    remaining_count = [0] * len(gold_heads)\n",
    "    for node in gold_heads:\n",
    "        remaining_count[node] += 1\n",
    "\n",
    "    # Simulate a parser\n",
    "    config = ArcStandardParser.initial_config(len(gold_heads))\n",
    "    while not ArcStandardParser.is_final_config(config):\n",
    "        pos, stack, heads = config\n",
    "        if len(stack) >= 2:\n",
    "            s1 = stack[-1]\n",
    "            s2 = stack[-2]\n",
    "            if gold_heads[s2] == s1 and remaining_count[s2] == 0:\n",
    "                move = ArcStandardParser.LA\n",
    "                yield config, move\n",
    "                config = ArcStandardParser.next_config(config, move)\n",
    "                remaining_count[s1] -= 1\n",
    "                continue\n",
    "            if gold_heads[s1] == s2 and remaining_count[s1] == 0:\n",
    "                move = ArcStandardParser.RA\n",
    "                yield config, move\n",
    "                config = ArcStandardParser.next_config(config, move)\n",
    "                remaining_count[s2] -= 1\n",
    "                continue\n",
    "        move = ArcStandardParser.SH\n",
    "        yield config, move\n",
    "        config = ArcStandardParser.next_config(config, move)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the training examples\n",
    "\n",
    "This time, we are generating the training examples for the parser:\n",
    "\n",
    "**training_examples** (*vocab_words*, *vocab_tags*, *gold_data*, *parser*, *batch_size* = 100)\n",
    "\n",
    "> Iterates through the given *gold_data* (an iterable of parsed sentences), encodes it into word ids and tag ids using the specified vocabularies *vocab_words* and *vocab_tags*, and then yields batches of training examples for gradient-based training. Each batch contains *batch_size* examples, except for the last batch, which may contain fewer examples. Each example in the batch is created by a call to the `featurize` function of the *parser*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_examples(vocab_words, vocab_tags, gold_data, parser, batch_size=100):\n",
    "    bx = []\n",
    "    by = []\n",
    "\n",
    "    for sentence in gold_data:\n",
    "        # Separate the words, gold tags, and gold heads\n",
    "        words, tags, gold_heads = zip(*sentence)\n",
    "\n",
    "        # Encode words and tags using the vocabularies\n",
    "        words = [vocab_words.get(w, UNK_IDX) for w in words]\n",
    "        tags = [vocab_tags[t] for t in tags]\n",
    "\n",
    "        # Call the oracle\n",
    "        for config, gold_move in oracle_moves(gold_heads):\n",
    "            bx.append(parser.featurize(words, tags, config))\n",
    "            by.append(gold_move)\n",
    "            if len(bx) >= batch_size:\n",
    "                bx = torch.stack(bx)\n",
    "                by = torch.LongTensor(by)\n",
    "                yield bx, by\n",
    "                bx = []\n",
    "                by = []\n",
    "\n",
    "    # Check whether there is an incomplete batch\n",
    "    if bx:\n",
    "        bx = torch.stack(bx)\n",
    "        by = torch.LongTensor(by)\n",
    "        yield bx, by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "The training loop is straightforward:\n",
    "\n",
    "**train_parser** (*train_data*, *n_epochs* = 1, *batch_size* = 100, *lr* = 1e-2)\n",
    "\n",
    "> Trains a fixed-window parser from a set of training data *train_data* (an iterable over parsed sentences) using minibatch gradient descent and returns it. The parameters *n_epochs* and *batch_size* specify the number of training epochs and the minibatch size, respectively. Training uses the cross-entropy loss function and the [Adam optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) with learning rate *lr*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_parser(train_data, n_epochs=1, batch_size=100, lr=1e-2):\n",
    "    # Create the vocabularies\n",
    "    vocab_words, vocab_tags = make_vocabs(train_data)\n",
    "\n",
    "    # Instantiate the parser\n",
    "    parser = FixedWindowParser(vocab_words, vocab_tags)\n",
    "\n",
    "    # Instantiate the optimizer\n",
    "    optimizer = optim.Adam(parser.model.parameters(), lr=lr)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0\n",
    "        n_examples = 0\n",
    "        with tqdm(total=sum(2*len(s)-1 for s in train_data)) as pbar:\n",
    "            for bx, by in training_examples(vocab_words, vocab_tags, train_data, parser):\n",
    "                optimizer.zero_grad()\n",
    "                output = parser.model.forward(bx)\n",
    "                loss = F.cross_entropy(output, by)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                n_examples += 1\n",
    "                pbar.set_postfix(loss=running_loss/n_examples)\n",
    "                pbar.update(len(bx))\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation function\n",
    "\n",
    "To evaluate a parser, we compute its unlabelled attachment score on gold-standard data. \n",
    "\n",
    "**uas** (*parser*, *gold_data*)\n",
    "\n",
    "> Computes the unlabelled attachment score of the specified *parser* on the gold-standard data *gold_data* (an iterable of parsed sentences) and returns it as a float. The unlabelled attachment score is the percentage of all tokens to which the parser assigns the correct head (as per the gold standard). The calculation ignores the pseudo-roots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uas(parser, gold_sentences):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for sentence in gold_sentences:\n",
    "        words, tags, gold_heads = zip(*sentence)\n",
    "        pred_heads = parser.predict(words, tags)\n",
    "        for gold, pred in zip(gold_heads[1:], pred_heads[1:]):  # ignore the pseudo-root\n",
    "            correct += int(gold == pred)\n",
    "            total += 1\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "The next code cell trains a tagger and evaluates it on the development data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARSER = train_parser(TRAIN_DATA, n_epochs=1)\n",
    "print('{:.4f}'.format(uas(PARSER, DEV_DATA)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unlabelled attachment score on the development data (with gold-standard tags) should be around 70%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Final evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final evaluation, we chain the tagger and the parser into a pipeline: The tags predicted by the tagger become the input tags to the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tagger, parser, gold_sentences):\n",
    "    correct_tagger = 0\n",
    "    total_tagger = 0\n",
    "    correct_parser = 0\n",
    "    total_parser = 0\n",
    "    for sentence in gold_sentences:\n",
    "        words, gold_tags, gold_heads = zip(*sentence)\n",
    "        pred_tags = tagger.predict(words)\n",
    "        for gold, pred in zip(gold_tags[1:], pred_tags[1:]):\n",
    "            correct_tagger += int(gold == pred)\n",
    "            total_tagger += 1\n",
    "        pred_heads = parser.predict(words, pred_tags)\n",
    "        for gold, pred in zip(gold_heads[1:], pred_heads[1:]):\n",
    "            correct_parser += int(gold == pred)\n",
    "            total_parser += 1\n",
    "    return correct_tagger / total_tagger, correct_parser / total_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, uas = evaluate(TAGGER, PARSER, DEV_DATA)\n",
    "print('acc: {:.4f}, uas: {:.4f}'.format(acc, uas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tagging accuracy and unlabelled attachment score on the development data should be around 88% and 65%, respectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
